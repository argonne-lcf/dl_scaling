:::MLLOG {"namespace": "", "time_ms": 1663009006663, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": null, "metadata": {"file": "./main.py", "lineno": 46, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009007016, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "./main.py", "lineno": 47, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009007016, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "cosmoflow", "metadata": {"file": "./main.py", "lineno": 51, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009007016, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "./main.py", "lineno": 53, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009007016, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "./main.py", "lineno": 55, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009007016, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "./main.py", "lineno": 57, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009007016, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "128.0xNVIDIA DGX A100", "metadata": {"file": "./main.py", "lineno": 59, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009007016, "event_type": "POINT_IN_TIME", "key": "number_of_nodes", "value": 128, "metadata": {"file": "./main.py", "lineno": 62, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009007017, "event_type": "POINT_IN_TIME", "key": "accelerators_per_node", "value": 4, "metadata": {"file": "./main.py", "lineno": 64, "instance": 0}}
[2022-09-12 13:56:51,792][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-12 13:56:51,799][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 512 nodes.
:::MLLOG {"namespace": "", "time_ms": 1663009011946, "event_type": "POINT_IN_TIME", "key": "dropout", "value": 0.5, "metadata": {"file": "./main.py", "lineno": 110, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009015062, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0, "metadata": {"file": "/lus/grand/projects/datascience/hzheng/mlperf-2022/optimized-hpc/cosmoflow/pytorch/optimizer.py", "lineno": 63, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009015062, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "sgd", "metadata": {"file": "/lus/grand/projects/datascience/hzheng/mlperf-2022/optimized-hpc/cosmoflow/pytorch/optimizer.py", "lineno": 65, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009015063, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.008, "metadata": {"file": "/lus/grand/projects/datascience/hzheng/mlperf-2022/optimized-hpc/cosmoflow/pytorch/optimizer.py", "lineno": 96, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009015063, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 4, "metadata": {"file": "/lus/grand/projects/datascience/hzheng/mlperf-2022/optimized-hpc/cosmoflow/pytorch/optimizer.py", "lineno": 98, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009015063, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 8, "metadata": {"file": "/lus/grand/projects/datascience/hzheng/mlperf-2022/optimized-hpc/cosmoflow/pytorch/optimizer.py", "lineno": 100, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009015063, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [32, 64], "metadata": {"file": "/lus/grand/projects/datascience/hzheng/mlperf-2022/optimized-hpc/cosmoflow/pytorch/optimizer.py", "lineno": 102, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009015063, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": [0.25, 0.125], "metadata": {"file": "/lus/grand/projects/datascience/hzheng/mlperf-2022/optimized-hpc/cosmoflow/pytorch/optimizer.py", "lineno": 104, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009015064, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 512, "metadata": {"file": "./main.py", "lineno": 124, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009015064, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 524288, "metadata": {"file": "./main.py", "lineno": 126, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009015064, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 65536, "metadata": {"file": "./main.py", "lineno": 128, "instance": 0}}
[2022-09-12 13:57:04,734][torch.nn.parallel.distributed][INFO] - Reducer buckets have been rebuilt in this iteration.
:::MLLOG {"namespace": "", "time_ms": 1663009034340, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/lus/grand/projects/datascience/hzheng/mlperf-2022/optimized-hpc/cosmoflow/pytorch/utils/utils.py", "lineno": 216, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009034340, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "./main.py", "lineno": 148, "instance": 0}}
:::MLLOG {"namespace": "", "time_ms": 1663009034341, "event_type": "INTERVAL_START", "key": "staging_start", "value": null, "metadata": {"file": "./main.py", "lineno": 151, "instance": 0}}
Staging non distributed files (1024)
Staging non distributed files (128)
:::MLLOG {"namespace": "", "time_ms": 1663009100310, "event_type": "INTERVAL_END", "key": "staging_stop", "value": null, "metadata": {"file": "/lus/grand/projects/datascience/hzheng/mlperf-2022/optimized-hpc/cosmoflow/pytorch/utils/utils.py", "lineno": 216, "staging_duration": 65.96900177001953, "instance": 0}}
/home/hzheng/.local/polaris/conda/2022-07-19/lib/python3.8/site-packages/nvidia/dali/plugin/base_iterator.py:192: Warning: Please set `reader_name` and don't set last_batch_padded and size manually whenever possible. This may lead, in some situations, to missing some samples or returning duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
